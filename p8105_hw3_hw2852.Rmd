---
title: "P8105 Data Science I Homework 3"
author: Olivia Wang (hw2852)
output: github_document
date: "2022-10-15"
---

In preparation for the problems below, we will load the following libraries: 

```{r load_libraries}
library(tidyverse)
library(readxl)
library(dplyr)
```

# Problem 1

This problem uses the `Instacart` data. This data set will be loaded through the `p8105.datasets` library. 

```{r}
library(p8105.datasets)
data("instacart")
```

## 1.1 Description of `Instacart` Data 


## 1.2 Analysis of `Instacart` Data

### Aisles

We can enumerate the number of aisles by applying the `group_by` function to identify the number of unique `aisle_id` variable values. The number of rows generated in the output would be the number of aisles in the data set. Building upon the results generated from the `group_by` function, we may determine the aisles from which the most items were ordered. This process involves generating a summary of the number of times each `aisle_id` appears in these data, then arranging the aisles in decreasing order of the number of times it appears. 

```{r}
instacart %>%
  group_by(aisle_id) %>% 
  summarise(items_ordered = n()) %>% 
  arrange(desc(items_ordered))
```

There are __134 aisles__ in the `Instacart` data set. Of the 134 aisles, the following are the aisles from which the most items are ordered:

Aisle Number   Number of Items Sold
-------------  --------------------
__83__         150,609
__24__         150,473
__123__        78,493
__120__        55,240
__21__         41,699

### Plotting Aisle vs. Items Ordered

```{r}
instacart %>% 
  group_by(aisle_id) %>% 
  mutate(items_ordered = n()) %>% 
  filter(items_ordered > 10000) %>% 
  ggplot(aes(x = aisle_id)) + 
  geom_histogram(color = "black", fill = "lightblue") + 
  labs(title = "Plot of Aisle by Numbers of Items Sold", x = "Aisle ID", y = "Number of Items Ordered")
```

# Problem 2

## 2.1 Accelerometer Data: Read, Tidy, Wrangle

We will begin by importing and cleaning the CSV file containing this patient's accelerometer data. This process involves data import, cleaning variable names, and applying the `pivot_longer` function to convert the data from wide to long format. A new `weekend_vs_weekday` variable was generated to indicate whether the entry corresponds to a weekend or a weekday.

```{r}
accelerometer_data = 
  read_csv("./accel_data.csv") %>% 
  janitor::clean_names(.) %>% 
  mutate(weekend_vs_weekday = if_else(day != "Saturday" & day != "Sunday","weekday", "weekend")) %>% 
  pivot_longer(
    activity_1:activity_1440, 
    names_to = "activity_time", 
    names_prefix = "activity_",
    names_transform = list(activity_time = as.integer),
    values_to = "activity_count")
```

*** Data description 

## 2.2 Total Daily Activity

Using the tidied accelerometer data generated in Part 2.1, we can aggregate across daily minutes to create a total activity variable for each of the 35 days of observation. We will first group the entries by `day_id`, then apply the `summarise` function to generate a new variable taking on the value of the sum of the activity counts associated with the specific `day_id`. 

```{r}
accelerometer_data %>% 
  group_by(day_id) %>% 
  summarise(total_daily_activity = sum(activity_count)) %>% 
  print(n = 35)
```

*** Data description

## 2.3 Plotting Accelerometer Daily Activity Count

We can apply the `ggplot` function to generate a single-panel plot depicting the 24-hour activity time courses for each of the 35 days of observation. In the line graph below, each day of observation's accelerometer activity count data is plotted against the corresponding minute of the day at which the reading was recorded, with the different colors delineating the day of the week. 

```{r}
accelerometer_data %>% 
  ggplot(aes(x = activity_time, y = activity_count, color = day)) +
  geom_line(aes(group = day_id)) +
  theme(
    legend.position = "bottom", 
    plot.title = element_text(hjust = 0.5)) +
  labs(
    title = "Plot of Accelerometer Daily Activity Count", 
    x = "Activity Time (Minute of Day)", 
    y = "Activity Count",
    color = "Day of Week") 
```

*** Data description

# Problem 3

This problem uses `NY NOAA` data. This data set will be loaded through the `p8105.datasets` library. 

```{r}
library(p8105.datasets)
data("ny_noaa")
```


## 3.1 NY NOAA Data: Tidying Data

We will first tidy the `ny_noaa` data set. This process involves cleaning variable names, creating new `year`, `month` and `day` variables from the existing `date` variable, and mutating year, month, date, precipitation, maximum and minimum temperature variables to appropriate (i.e., numeric) units). 

```{r}
ny_noaa_data = ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(col = date, into = c('year', 'month','day'), sep = '-') %>%
  mutate(
    year = as.numeric(year),
    month = as.numeric(month),
    day = as.numeric(day),
    prcp = prcp/10, 
    tmax = as.numeric(tmax) / 10,
    tmin = as.numeric(tmin) / 10
  )
```

## 3.2 Snowfall 

Using the tidied `ny_noaa_data` data set, we can determine the most commonly observed values of snowfall by first grouping the observations by `snow`, then summarizing the number of observations for each `snow` value, and arranging the output in descending order. 

```{r}
ny_noaa_data %>% 
  group_by(snow) %>% 
  summarise(snowfall_n_obs = n()) %>% 
  arrange(desc(snowfall_n_obs))
```
## 3.3 Plotting Maximum Temperatures in January & July

To generate a 2-panel plot depicting the average maximum temperature in January and July in each station across years we must first generate a 

```{r}
ny_noaa_data %>%
  select(id, year, month, day, tmax) %>% 
  filter(month == 1 | month == 7) %>% 
  ggplot(aes(x = year, y = tmax)) +
  geom_point(alpha = 0.05) +
  geom_smooth(se = FALSE) +
  facet_grid(~month)
```



