---
title: "P8105 Data Science I Homework 3"
author: Olivia Wang (hw2852)
output: github_document
date: "2022-10-15"
---

In preparation for the problems below, we will load the following libraries: 

```{r load_libraries}
library(tidyverse)
library(readxl)
library(dplyr)
library(patchwork)
```

# Problem 1

This problem uses the `Instacart` data. This data set will be loaded through the `p8105.datasets` library. 

```{r}
library(p8105.datasets)
data("instacart")
```

## 1.1 Description of `Instacart` Data 

The `instacart` data set contains __`r nrow(instacart)` rows__ and __`r ncol(instacart)` columns__. Each row in the data set corresponds to a single item of an Instacart order. The variables in the data set include ID numbers corresponding to orders, products, and Instacart users, as well as the order in which a particular product was added to cart. The data set contains a number of order-level variables that describe the date and time an order was made, and the number of days since a previous order was made. There are also several item-specific variables that describe the product name, department to which the product belongs, the aisle in which the product can be found, and relevant order history. 

There are __`r instacart %>% select(product_id) %>% distinct %>% count` products__ found in __`r instacart %>% select(user_id, order_id) %>% distinct %>% count` orders__ from __`r instacart %>% select(user_id) %>% distinct %>% count` distinct users__.

## 1.2 Analysis of `Instacart` Data

### Enumerating Total Aisles

We can enumerate the number of aisles by applying the `group_by` function to identify the number of unique `aisle_id` variable values. The number of rows generated in the output would be the number of aisles in the data set. Building upon the results generated from the `group_by` function, we may determine the aisles from which the most items were ordered. This process involves generating a summary of the number of times each `aisle_id` appears in these data, then arranging the aisles in decreasing order of the number of times it appears. 

```{r}
instacart %>%
  group_by(aisle_id) %>% 
  summarise(items_ordered = n()) %>% 
  arrange(desc(items_ordered))
```

There are __134 aisles__ in the `Instacart` data set. Of the 134 aisles, the following are the aisles from which the most items are ordered:

Aisle Number   Number of Items Sold
-------------  --------------------
__83__         150,609
__24__         150,473
__123__        78,493
__120__        55,240
__21__         41,699

### Plotting Aisle vs. Items Ordered

Using the `instacart` data set, we can generate a plot showing the number of items ordered in each aisle, limiting this to aisles with more than 10,000 items ordered. This process involves counting the number of distinct aisles and filtering the data to only include aisles in which more than 10,000 items are ordered. We can then generate a scatter plot using `ggplot`, with the aisles arranged in increasing order of number of items ordered. 

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(
    title = "Number of Items Ordered per Aisle", 
    x = "Aisle", 
    y = "Number of Items Ordered (n)") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1), 
        plot.title = element_text(hjust = 0.5))
```

### Top 3 Most Popular Items per Aisle

The following table identifies the top 3 most popular items in the `baking ingredients`, `dog food care` and `packaged vegetables fruits` aisles, and enumerates the times each item is ordered. 

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```

### Mean Hour of Purchases: Pink Lady Apples & Coffe Ice Cream

This final table displays the mean hour of the day at which the Pink Lady Applies and Coffee Ice Cream products are ordered each day of the week.

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)
```

Based on the table generated above, we can conclude that Pink Lady Apples are usually purchased earlier in the day than Coffee Ice Cream, with the exception of Day 5. 

# Problem 2

## 2.1 Accelerometer Data: Read, Tidy, Wrangle

We will begin by importing and cleaning the CSV file containing this patient's accelerometer data. This process involves data import, cleaning variable names, and applying the `pivot_longer` function to convert the data from wide to long format. A new `weekend_vs_weekday` variable was generated to indicate whether the entry corresponds to a weekend or a weekday.

```{r}
accelerometer_data = 
  read_csv("./accel_data.csv") %>% 
  janitor::clean_names(.) %>% 
  mutate(weekend_vs_weekday = if_else(day != "Saturday" & day != "Sunday","weekday", "weekend")) %>% 
  pivot_longer(
    activity_1:activity_1440, 
    names_to = "activity_time", 
    names_prefix = "activity_",
    names_transform = list(activity_time = as.integer),
    values_to = "activity_count")
```
The tidied `accelerometer_data` data set contains __`r nrow(accelerometer_data)` rows__ and __`r ncol(accelerometer_data)` columns__. Each row in the data set corresponds to a single reading of accelerometer activity. The variables in the data set include identifiers of the week, day, and minute of the day at which the accelerometer activity count is recorded. The newly created `weekend_vs_weekday` variable also identifies whether the reading corresponds to a weekend or a weekday. Accelerometer activity count data is collected on __`r accelerometer_data %>% select(day_id) %>% distinct %>% count` days__ over __`r accelerometer_data %>% select(week) %>% distinct %>% count` weeks__. 

## 2.2 Total Daily Activity

Using the tidied accelerometer data generated in Part 2.1, we can aggregate across daily minutes to create a total activity variable for each of the 35 days of observation. We will first group the entries by `day_id`, then apply the `summarise` function to generate a new variable taking on the value of the sum of the activity counts associated with the specific `day_id`. 

```{r}
accelerometer_data %>% 
  group_by(day_id) %>% 
  mutate(total_daily_activity = sum(activity_count)) %>% 
  summarise(day, total_daily_activity) %>% 
  distinct %>% 
  print(n = 35) %>% 
  knitr::kable()
```

It is difficult to identify any apparent trend through the tabulated data alone. Based on a cursory glance, we can see that on days 24 and 31, both of which are Saturdays, the accelerometer activity count at every minute of the day were 1, thus aggregating to a total of 1440 over 24 hours for both those days. To more effectively identify trends in these data, it would be helpful to generate a scatter or line plot to view how the accelerometer activity count data varies over time. 

## 2.3 Plotting Accelerometer Daily Activity Count

We can apply the `ggplot` function to generate a single-panel plot depicting the 24-hour activity time courses for each of the 35 days of observation. In the line graph below, each day of observation's accelerometer activity count data is plotted against the corresponding minute of the day at which the reading was recorded, with the different colors delineating the day of the week. 

```{r}
accelerometer_data %>% 
  ggplot(aes(x = activity_time, y = activity_count, color = day)) +
  geom_line(aes(group = day_id)) +
  theme(
    legend.position = "bottom", 
    plot.title = element_text(hjust = 0.5)) +
  labs(
    title = "Daily Accelerometer Activity Count", 
    x = "Activity Time (Minute of Day)", 
    y = "Activity Count",
    color = "Day of Week") 
```

Based on the line plot generated above, we can see that activity counts are lower during hours of the day at which the patient under observation is likely asleep. More specifically, between the hours of around 9:30pm (minute 1300) to 4:00am (minute 250), accelerometer activity count are consistently lower than during other times of the day, and there are also fewer outlying jumps in activity count during those times. Furthermore, these data also show increased average accelerometer activity counts during the hours of around 7:00pm (minute 1170) to 10:30pm (minute 1350). There does not seem to be a specific day of the week during which sudden increases in activity count is more frequent. The main trends observable in these data are mainly with respect to the time of day during which increases or decreases of activity count are apparent. 

# Problem 3

This problem uses `NY NOAA` data. This data set will be loaded through the `p8105.datasets` library. 

```{r}
library(p8105.datasets)
data("ny_noaa")
skimr::skim(ny_noaa)
```

The untidied `ny_noaa` data set contains __`r nrow(ny_noaa)` rows__ and __`r ncol(ny_noaa)` columns__. Each row in the data set includes the snowfall, snow depth, maximum and minimum temperature readings corresponding to a specific weather station on a specific date. 

There is no missing data for the `id` and `date` variables. Based on the data summary generated above, there are over 1 million missing values for each of maximum and minimum temperature variables. In other words, temperature measures are missing for approximately half of the entries. Due to the significant amount of missing temperature data, further analyses conducted with temperature data may generate invalid conclusions. There is also missing data for the precipitation, snowfall and snow depth variables; however, each are missing significantly fewer entries compared to the number of missing temperature measures, namely `r sum(is.na(ny_noaa$prcp))` missing values for precipitation, `r sum(is.na(ny_noaa$snow))` missing values for snowfall, and `r sum(is.na(ny_noaa$snwd))` missing values for snow depth. Missing values of precipitation, snowfall and snow depth pose significantly lesser issues for data analysis, since they account for a lower percentage of total entries. 

## 3.1 NY NOAA Data: Tidying Data

We will first tidy the `ny_noaa` data set. This process involves cleaning variable names, creating new `year`, `month` and `day` variables from the existing `date` variable, and mutating year, month, date, precipitation, maximum and minimum temperature variables to appropriate (i.e., numeric) units). 

```{r}
ny_noaa_data = ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(col = date, into = c('year', 'month','day'), sep = '-') %>%
  mutate(
    year = as.numeric(year),
    month = as.numeric(month),
    day = as.numeric(day),
    prcp = prcp/10, 
    tmax = as.numeric(tmax) / 10,
    tmin = as.numeric(tmin) / 10
  )
```

## 3.2 Snowfall 

Using the tidied `ny_noaa_data` data set, we can determine the most commonly observed values of snowfall by first grouping the observations by `snow`, then summarizing the number of observations for each `snow` value, and arranging the output in descending order. 

```{r}
ny_noaa_data %>% 
  group_by(snow) %>% 
  summarise(snowfall_n_obs = n()) %>% 
  arrange(desc(snowfall_n_obs)) %>% 
  knitr::kable() 
```

The most commonly observed values for snowfall are 0mm, NA, 25mm, 13mm and 51mm. It is not surprising that 0mm is the most commonly observed value for snowfall, since New York usually does not have snow for most of the year: New York usually only expects snow during the months of December through February. The missing values (NA) generated in the table above are consistent with the number of missing values identified in the previously generated data summary. The subsequently most commonly observed values of 25mm, 13mm and 51mm. These measures are consistent with the fact that New York typically does not have a lot of snowfall during the winter seasons.

## 3.3 Plotting Maximum Temperatures in January & July

Using the tidied `ny_noaa_data` data set, we can generate a 2-panel plot depicting the average maximum temperatures in January and July at each weather station across years. 

```{r}
ny_noaa_data %>%
  select(id, year, month, day, tmax) %>% 
  drop_na(tmax) %>% 
  filter(month == 1 | month == 7) %>% 
  group_by(year, month, id) %>% 
  mutate(month = recode(month, 
                        '1' = 'January', 
                        '7' = 'July'),
         mean_tmax = mean(tmax)) %>% 
  ggplot(aes(x = year, y = mean_tmax)) +
  geom_point(aes(color = tmax)) +
  geom_smooth(se = FALSE, color = "yellow") +
  facet_grid(~month) +
  labs(
    title = "Average Maximum Temperatures in January vs. July (1981-2010)",
    x = "Year", 
    y = "Average Maximum Temperature (C)", 
    color = "Temperature (C)") +
  theme(
    plot.title = element_text(hjust = 0.5),
    strip.text = element_text(face = "bold"))
```

As expected, the average maximum temperatures in January, a winter month, are consistently lower than the average maximum temperatures in July, a summer month. The variance in average maximum temperatures in January is greater than that of the average maximum temperatures in July. This is consistent with what we generally understand of January as more of a transition month, where the weather is slowly, but surely, transitioning from winter to spring. here are a few outliers in both January and July, most of which are usually colder than average temperatures; however, none are particularly alarming, or would raise concerns about potential measurement error. 

## 3.4 Plotting Maximum and Minimum Temperatures & Distribution of Snowfall

Finally, we can generate another 2-panel plot showing (i) the density of maximum and minimum temperatures for the full dataset; and (ii) the distribution of snowfall values greater than 0 and less than 100 separately by year.  

```{r}
tmax_vs_tmin_p = 
ny_noaa_data %>% 
  select(tmax,tmin) %>% 
  drop_na(tmax, tmin) %>% 
  pivot_longer(
    tmax:tmin,
    names_to = "temp_observation", 
    values_to = "temp_measurement") %>%
  mutate(temp_observation = recode(temp_observation, 
                                   'tmax' = "Max Temperature",
                                   'tmin' = "Min Temperature")) %>% 
  ggplot(aes(x = temp_measurement, fill = temp_observation)) +
  geom_density(alpha = 0.5) +
  labs(
    title = "Maximum & Minimum Temperatures",
    x = "Temperature (C)", 
    y = "Density", 
    fill = "Observation") +
   theme(
    legend.position = "bottom",
    plot.title = element_text(hjust = 0.5))

dist_snowfall_plot = 
  ny_noaa_data %>% 
  filter(0 < snow & snow < 100) %>% 
  group_by(year) %>% 
  ggplot(aes(x = snow, y = year, group = year, fill = year)) + 
  ggridges::geom_density_ridges() + 
  viridis::scale_fill_viridis() +
  labs(
    title = "Distribution of Snowfall",
    x = "Snowfall (mm)", 
    y = "Year", 
    fill = "year") +
   theme(
    legend.position = "none",
    plot.title = element_text(hjust = 0.5))   

tmax_vs_tmin_p + dist_snowfall_plot
```