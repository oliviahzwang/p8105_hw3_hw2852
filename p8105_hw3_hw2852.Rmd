---
title: "P8105 Data Science I Homework 3"
author: Olivia Wang (hw2852)
output: github_document
date: "2022-10-15"
---

In preparation for the problems below, we will load the following libraries: 

```{r load_libraries}
library(tidyverse)
library(readxl)
library(dplyr)
library(patchwork)
```

# Problem 1

This problem uses the `Instacart` data. This data set will be loaded through the `p8105.datasets` library. 

```{r}
library(p8105.datasets)
data("instacart")
```

## 1.1 Description of `Instacart` Data 

The `instacart` data set contains __`r nrow(instacart)` rows__ and __`r ncol(instacart)` columns__. Each row in the data set corresponds to a single item of an Instacart order. The variables in the data set include ID numbers corresponding to orders, products, and Instacart users, as well as the order in which a particular product was added to cart. The data set contains a number of order-level variables that describe the date and time an order was made, and the number of days since a previous order was made. There are also several item-specific variables that describe the product name, department to which the product belongs, the aisle in which the product can be found, and relevant order history. 

There are __`r instacart %>% select(product_id) %>% distinct %>% count` products__ found in __`r instacart %>% select(user_id, order_id) %>% distinct %>% count` orders__ from __`r instacart %>% select(user_id) %>% distinct %>% count` distinct users__.

## 1.2 Analysis of `Instacart` Data

### Enumerating Total Aisles

We can enumerate the number of aisles by applying the `group_by` function to identify the number of unique `aisle_id` variable values. The number of rows generated in the output would be the number of aisles in the data set. Building upon the results generated from the `group_by` function, we may determine the aisles from which the most items were ordered. This process involves generating a summary of the number of times each `aisle_id` appears in these data, then arranging the aisles in decreasing order of the number of times it appears. 

```{r}
instacart %>%
  group_by(aisle_id) %>% 
  summarise(items_ordered = n()) %>% 
  arrange(desc(items_ordered))
```

There are __134 aisles__ in the `Instacart` data set. Of the 134 aisles, the following are the aisles from which the most items are ordered:

Aisle Number   Number of Items Sold
-------------  --------------------
__83__         150,609
__24__         150,473
__123__        78,493
__120__        55,240
__21__         41,699

### Plotting Aisle vs. Items Ordered

Using the `instacart` data set, we can generate a plot showing the number of items ordered in each aisle, limiting this to aisles with more than 10,000 items ordered. This process involves counting the number of distinct aisles and filtering the data to only include aisles in which more than 10,000 items are ordered. We can then generate a scatter plot using `ggplot`, with the aisles arranged in increasing order of number of items ordered. 

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(
    title = "Number of Items Ordered per Aisle", 
    x = "Aisle", 
    y = "Number of Items Ordered (n)") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1), 
        plot.title = element_text(hjust = 0.5))
```

### Top 3 Most Popular Items per Aisle

The following table identifies the top 3 most popular items in the `baking ingredients`, `dog food care` and `packaged vegetables fruits` aisles, and enumerates the times each item is ordered. 

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```

### Mean Hour of Purchases: Pink Lady Apples & Coffe Ice Cream

This final table displays the mean hour of the day at which the Pink Lady Applies and Coffee Ice Cream products are ordered each day of the week.

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)
```

Based on the table generated above, we can conclude that Pink Lady Apples are usually purchased earlier in the day than Coffee Ice Cream, with the exception of Day 5. 

# Problem 2

## 2.1 Accelerometer Data: Read, Tidy, Wrangle

We will begin by importing and cleaning the CSV file containing this patient's accelerometer data. This process involves data import, cleaning variable names, and applying the `pivot_longer` function to convert the data from wide to long format. A new `weekend_vs_weekday` variable was generated to indicate whether the entry corresponds to a weekend or a weekday.

```{r}
accelerometer_data = 
  read_csv("./accel_data.csv") %>% 
  janitor::clean_names(.) %>% 
  mutate(weekend_vs_weekday = if_else(day != "Saturday" & day != "Sunday","weekday", "weekend")) %>% 
  pivot_longer(
    activity_1:activity_1440, 
    names_to = "activity_time", 
    names_prefix = "activity_",
    names_transform = list(activity_time = as.integer),
    values_to = "activity_count")
```
The tidied `accelerometer_data` data set contains __`r nrow(accelerometer_data)` rows__ and __`r ncol(accelerometer_data)` columns__. Each row in the data set corresponds to a single reading of accelerometer activity. The variables in the data set include identifiers of the week, day, and minute of the day at which the accelerometer activity count is recorded. The newly created `weekend_vs_weekday` variable also identifies whether the reading corresponds to a weekend or a weekday. Accelerometer activity count data is collected on __`r accelerometer_data %>% select(day_id) %>% distinct %>% count` days__ over __`r accelerometer_data %>% select(week) %>% distinct %>% count` weeks__. 

## 2.2 Total Daily Activity

Using the tidied accelerometer data generated in Part 2.1, we can aggregate across daily minutes to create a total activity variable for each of the 35 days of observation. We will first group the entries by `day_id`, then apply the `summarise` function to generate a new variable taking on the value of the sum of the activity counts associated with the specific `day_id`. 

```{r}
accelerometer_data %>% 
  group_by(day_id) %>% 
  mutate(total_daily_activity = sum(activity_count)) %>% 
  summarise(day, total_daily_activity) %>% 
  distinct %>% 
  print(n = 35) %>% 
  knitr::kable()
```

It is difficult to identify any apparent trend through the tabulated data alone. Based on a cursory glance, we can see that on days 24 and 31, both of which are Saturdays, the accelerometer activity count at every minute of the day were 1, thus aggregating to a total of 1440 over 24 hours for both those days. To more effectively identify trends in these data, it would be helpful to generate a scatter or line plot to view how the accelerometer activity count data varies over time. 

## 2.3 Plotting Accelerometer Daily Activity Count

We can apply the `ggplot` function to generate a single-panel plot depicting the 24-hour activity time courses for each of the 35 days of observation. In the line graph below, each day of observation's accelerometer activity count data is plotted against the corresponding minute of the day at which the reading was recorded, with the different colors delineating the day of the week. 

```{r}
accelerometer_data %>% 
  ggplot(aes(x = activity_time, y = activity_count, color = day)) +
  geom_line(aes(group = day_id)) +
  theme(
    legend.position = "bottom", 
    plot.title = element_text(hjust = 0.5)) +
  labs(
    title = "Plot of Accelerometer Daily Activity Count", 
    x = "Activity Time (Minute of Day)", 
    y = "Activity Count",
    color = "Day of Week") 
```

Based on the line plot generated above, we can see that activity counts are lower during hours of the day at which the patient under observation is likely asleep. More specifically, between the hours of around 9:30pm (minute 1300) to 4:00am (minute 250), accelerometer activity count are consistently lower than during other times of the day, and there are also fewer outlying jumps in activity count during those times. Furthermore, these data also show increased average accelerometer activity counts during the hours of around 7:00pm (minute 1170) to 10:30pm (minute 1350). There does not seem to be a specific day of the week during which sudden increases in activity count is more frequent. The main trends observable in these data are mainly with respect to the time of day during which increases or decreases of activity count are apparent. 

# Problem 3

This problem uses `NY NOAA` data. This data set will be loaded through the `p8105.datasets` library. 

```{r}
library(p8105.datasets)
data("ny_noaa")
```


## 3.1 NY NOAA Data: Tidying Data

We will first tidy the `ny_noaa` data set. This process involves cleaning variable names, creating new `year`, `month` and `day` variables from the existing `date` variable, and mutating year, month, date, precipitation, maximum and minimum temperature variables to appropriate (i.e., numeric) units). 

```{r}
ny_noaa_data = ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(col = date, into = c('year', 'month','day'), sep = '-') %>%
  mutate(
    year = as.numeric(year),
    month = as.numeric(month),
    day = as.numeric(day),
    prcp = prcp/10, 
    tmax = as.numeric(tmax) / 10,
    tmin = as.numeric(tmin) / 10
  )
```

## 3.2 Snowfall 

Using the tidied `ny_noaa_data` data set, we can determine the most commonly observed values of snowfall by first grouping the observations by `snow`, then summarizing the number of observations for each `snow` value, and arranging the output in descending order. 

```{r}
ny_noaa_data %>% 
  group_by(snow) %>% 
  summarise(snowfall_n_obs = n()) %>% 
  arrange(desc(snowfall_n_obs))
```
## 3.3 Plotting Maximum Temperatures in January & July

To generate a 2-panel plot depicting the average maximum temperature in January and July in each station across years. 

```{r}
ny_noaa_data %>%
  select(id, year, month, day, tmax) %>% 
  filter(month == 1 | month == 7) %>% 
  mutate(month = recode(month, 
                        '1' = 'January', 
                        '7' = 'July')) %>% 
  ggplot(aes(x = year, y = tmax)) +
  geom_point(aes(color = tmax)) +
  geom_smooth(se = FALSE, color = "yellow") +
  facet_grid(~month) +
  labs(
    title = "Maximum Temperatures in January vs. July (1981-2010)",
    x = "Year", 
    y = "Maximum Temperature (C)", 
    color = "Temperature (C)") +
  theme(
    plot.title = element_text(hjust = 0.5),
    strip.text = element_text(face = "bold"))
```

## 3.4 Plotting Maximum and Minimum Temperatures & Distribution of Snowfall

```{r}
tmax_vs_tmin_p = 
ny_noaa_data %>% 
  select(tmax,tmin) %>% 
  drop_na(tmax, tmin) %>% 
  pivot_longer(
    tmax:tmin,
    names_to = "temp_observation", 
    values_to = "temp_measurement") %>%
  mutate(temp_observation = recode(temp_observation, 
                                   'tmax' = "Max Temperature",
                                   'tmin' = "Min Temperature")) %>% 
  ggplot(aes(x = temp_measurement, fill = temp_observation)) +
  geom_density(alpha = 0.5) +
  labs(
    title = "Maximum & Minimum Temperatures",
    x = "Temperature (C)", 
    y = "Density", 
    fill = "Observation") +
   theme(
    legend.position = "bottom",
    plot.title = element_text(hjust = 0.5))

dist_snowfall_plot = 
  ny_noaa_data %>% 
  filter(0 < snow & snow < 100) %>% 
  group_by(year) %>% 
  ggplot(aes(x = snow, y = year, group = year, fill = year)) + 
  ggridges::geom_density_ridges() + 
  viridis::scale_fill_viridis() +
  labs(
    title = "Distribution of Snowfall",
    x = "Snowfall (mm)", 
    y = "Density", 
    fill = "year") +
   theme(
    legend.position = "none",
    plot.title = element_text(hjust = 0.5))   

tmax_vs_tmin_p + dist_snowfall_plot

```


